{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m#[0.0s]---Importations réussies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Importations\n",
    "import time\n",
    "t1 = time.time()\n",
    "try:\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import numpy as np\n",
    "    from dateutil.parser import parse as dateparse\n",
    "    from lat_lon_parser import parse as parse\n",
    "    import json\n",
    "    import re\n",
    "    import reverse_geocode as rg\n",
    "    import pycountry\n",
    "    from pyarrow import fs\n",
    "    import multiprocessing\n",
    "    import dask.dataframe as ddf\n",
    "    import datetime\n",
    "    import threading\n",
    "\n",
    "    import sys\n",
    "    sys.path.append('../representations')\n",
    "    import graph_lib as graph\n",
    "\n",
    "    import globals as p\n",
    "\n",
    "    t2 = time.time()\n",
    "    t = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Importations réussies\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    t2 = time.time()\n",
    "    t = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur d'importations\",time=t,error_type=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m# Ouverture du fichier des métadonnées        \n",
      "\u001b[92m#[46.9s]---Ouverture à distance réussie\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ouverture des métadonnées\n",
    "p.set_done(False)\n",
    "t1 = time.time()\n",
    "\n",
    "thr = threading.Thread(target=graph.animate_string_execution(\"Ouverture du fichier des métadonnées\"))\n",
    "thr.start()\n",
    "\n",
    "try:\n",
    "    \n",
    "    s3fs = fs.S3FileSystem(\n",
    "        endpoint_override=\"localhost:9000\",\n",
    "        access_key=\"lbIwWP4FIBtjBwU5AaI0\",\n",
    "        secret_key=\"nw54mjkG3CjxjvkyqxACTbYOuhtzyc1YmkMaJXeL\",\n",
    "        scheme=\"http\"\n",
    "    )\n",
    "    df = pd.read_parquet('geo-sra/geo-sra-to-ecoregions.parquet.gzip',filesystem=s3fs)\n",
    "\n",
    "    p.set_done(True)\n",
    "\n",
    "    type_open = \"dist\"\n",
    "\n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Ouverture à distance réussie\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    type_open = \"loc\"\n",
    "    \n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur d'ouverture à distance\",time=t,error_type=e)\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = pd.read_parquet(\"/Users/tpietav/Desktop/data/raw/sra_metadata_toy.parquet\")\n",
    "\n",
    "        p.set_done(True)\n",
    "\n",
    "        print('')\n",
    "        t2 = time.time()\n",
    "        t  = np.round(t2 - t1,decimals=1)\n",
    "        graph.print_cell_msg(\"Ouverture locale réussie\",time=t,error=False)\n",
    "    \n",
    "    except Exception as e_local:\n",
    "\n",
    "        p.set_done(True)\n",
    "\n",
    "        print('')\n",
    "        t2 = time.time()\n",
    "        t  = np.round(t2 - t1,decimals=1)\n",
    "        graph.print_cell_msg(\"Erreur d'ouverture locale\",time=t,error_type=e_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire des pays et leur alpha 3\n",
    "t1 = time.time()\n",
    "try:\n",
    "\n",
    "    country_name_to_alpha_3 = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "\n",
    "    additional_mappings = {\n",
    "        \"Svalbard and Jan Mayen\":\"NOR\",\n",
    "        \"South Korea\":\"KOR\",\n",
    "        \"Czech Republic\":\"CZE\",\n",
    "        \"Russian Federation\":\"RUS\",\n",
    "        \"Russia\":\"RUS\",\n",
    "        \"Hong Kong\":\"CHN\",\n",
    "        \"Vietnam\":\"VNM\",\n",
    "        \"Taiwan\":\"TWN\",\n",
    "        \"Palestinian Territory\":\"ISR\",\n",
    "        \"Turkey\":\"TUR\",\n",
    "        \"Bolivia\":\"BOL\",\n",
    "        \"Libyan Arab Jamahiriya\":\"LBY\",\n",
    "        \"Cape Verde\":\"CPV\",\n",
    "        \"Aland Islands\":\"ALA\",\n",
    "        \"Saint Helena\":\"SHN\",\n",
    "        \"Democratic Republic of the Congo\":\"COD\",\n",
    "        \"Laos\":\"LAO\",\n",
    "        \"Tanzania\":\"TZN\",\n",
    "        \"Brunei\":\"BRN\",\n",
    "        \"Iran\":\"IRN\",\n",
    "        \"Svalbard\":\"NOR\",\n",
    "        \"Macau\":\"CHN\",\n",
    "    }       \n",
    "    country_name_to_alpha_3.update(additional_mappings)\n",
    "\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Création du dictionnaire des pays réussie\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "    \n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur de création du dictionnaire des pays\",time=t,error_type=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"returns if a string is a date or not\n",
    "\n",
    "    Args:\n",
    "        string (string): string to check\n",
    "        fuzzy (bool, optional): fuzzy string or not. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        boolean: if the string is a date or not\n",
    "    \"\"\"\n",
    "    try: \n",
    "        dateparse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def select_first_collection_date(x):\n",
    "    \"\"\"select the first collection date of object x\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): series of collection date issue from a dataframe\n",
    "\n",
    "    Returns:\n",
    "        string: first collection date given for x\n",
    "    \"\"\"\n",
    "    if not pd.isnull(x):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_alpha_string(string,max_words=2):\n",
    "    \"\"\"return wether a string is an alpha string or not\n",
    "\n",
    "    Args:\n",
    "        string (string): string to check \n",
    "        max_words (int, optional): max words that can contain the string. Defaults to 2.\n",
    "    \"\"\"\n",
    "    for n in range(max_words):\n",
    "        pattern = r'^[a-zA-Z]+ {'+str(n)+r'}[a-zA-Z]+$'\n",
    "        if re.match(pattern,string):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def decimal_precision(nombre):\n",
    "    \"\"\"return the decimal precision of a number\n",
    "\n",
    "    Args:\n",
    "        nombre (float): number to calc\n",
    "\n",
    "    Returns:\n",
    "        int: number of decimals given in the number\n",
    "    \"\"\"\n",
    "    if nombre == np.NaN:\n",
    "        return np.NaN\n",
    "    chaine = str(nombre)\n",
    "    index_point = chaine.find('.')\n",
    "    if index_point == -1:\n",
    "        return np.NaN\n",
    "    return len(chaine) - index_point - 1\n",
    "\n",
    "def country_is_na(country):\n",
    "    \"\"\"return wether a country field is NA or not\n",
    "\n",
    "    Args:\n",
    "        country (string): Country value\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string is Na False if not\n",
    "    \"\"\"\n",
    "    country_na_tag = ['uncalculated','<NA>']\n",
    "    if pd.isna(country):\n",
    "        return True\n",
    "    elif country in country_na_tag:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_countries(country_name_1,country_name_2):\n",
    "    \"\"\"say if two string of countries matches or not\n",
    "\n",
    "    Args:\n",
    "        country_name_1 (string): string name of country one\n",
    "        country_name_2 (string): string name of country two\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the two countries are the same\n",
    "    \"\"\"\n",
    "    alpha_country_1 = country_name_to_alpha_3.get(country_name_1, None)\n",
    "    alpha_country_2 = country_name_to_alpha_3.get(country_name_2, None)\n",
    "    if alpha_country_1 == alpha_country_2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def reverse_lat_lon_test(longitude,latitude,country):\n",
    "    \"\"\"say if reversing coodinates matches the country given\n",
    "\n",
    "    Args:\n",
    "        longitude (float): longitude of the point\n",
    "        latitude (float): latitude of the point\n",
    "        country (string): string name of the country given\n",
    "\n",
    "    Returns:\n",
    "        bool: True if reversing coordinates matches the country\n",
    "    \"\"\"\n",
    "    coordinates = [(longitude,latitude)]\n",
    "    alpha3_country = country_name_to_alpha_3.get(country, None)\n",
    "    rgeocode = rg.search(coordinates)          \n",
    "    if(len(rgeocode)>0):\n",
    "        rg_country = rgeocode[0]['country']\n",
    "        alpha3_rg_country = country_name_to_alpha_3.get(rg_country, None)\n",
    "    if alpha3_country == alpha3_rg_country:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de collection_date\n",
    "t1 = time.time()\n",
    "try:\n",
    "\n",
    "    df[\"collection_date\"] = df['collection_date_sam'].apply(select_first_collection_date)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Calcul de collection_date\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur de calcul de collection_date\",time=t,error_type=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dask dataframe\n",
    "p.set_done(False)\n",
    "t1 = time.time()\n",
    "\n",
    "thr = threading.Thread(target=graph.animate_string_execution(\"Création du dask dataframe\"))\n",
    "thr.start()\n",
    "\n",
    "try:\n",
    "\n",
    "    num_partitions = multiprocessing.cpu_count()\n",
    "    dask = ddf.from_pandas(df, npartitions=num_partitions-5)\n",
    "\n",
    "    p.set_done(True)\n",
    "\n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Création du dask dataframe\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    p.set_done(True)\n",
    "\n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur de création du dask dataframe\",time=t,error_type=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataframe des métadonées finales\n",
    "t1 = time.time()\n",
    "try:\n",
    "\n",
    "    columns = ['bioproject', 'acc','organism','assay_type','instrument','librarylayout','libraryselection','librarysource','geo_loc_name_country_calc','geo_loc_name_country_continent_calc','mbytes','mbases','releasedate', 'collection_date','lat_lon_src','lat_lon_raw','latitude','longitude','latitude_precision','has_latlon','longitude_precision','rg_country_code','rg_city','rg_country','GEO_QUAL']\n",
    "    dtype = [str,str,str,str,str,str,str,str,str,str,int,int,datetime.date,datetime.date,str,str,float,float,float,float,bool,str,str,str,float,int,int,str]\n",
    "    cdt={i[0]: i[1] for i in zip(columns, dtype)}\n",
    "    meta_df = pd.DataFrame(columns=list(cdt))\n",
    "\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Création du dataframe des métadonnées\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur de création du dataframe des métadonnées\",time=t,error_type=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAGS TEST\n",
    "\n",
    "dic = {\n",
    "    \"Lat Lon Tag\":0,\n",
    "    \"Lat Tag\"    :0,\n",
    "    \"Lon Tag\"    :0,\n",
    "    \"No Tag\"     :0\n",
    "}\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# \n",
    "# selected tag\n",
    "#\n",
    "\n",
    "lat_lon_tag = ['lat_lon_sam_s_dpl34','lat_lon_sam_s_dpl1','geographic_location__latitude_and_longitude__sam','geographic_location__latitudeandlongitude__sam','latitude__and_longitude_sam','latitude_and_longitude_sam','lattitude_and_logitude_sam','lat_lon_sam','latlon_sam','location_coordinates_sam','other_gps_coordinates_sam','lat_lon_dms_sam','lat_long_correct_sam','lat_lon_run']\n",
    "\n",
    "lat_tag = ['geographic_location__latitude__sam_s_dpl4','lat_lon_sam_s_dpl1','latitude_sam','lat_sam','geographic_location__latitude__sam','geographiclocation_latitude__sam','latitude_dd_sam','latitude_deg_sam','biological_material_latitude_sam']\n",
    "\n",
    "lon_tag = ['geographic_location__longitude__sam_s_dpl5','longitude_sam','lon_sam','geographic_location__longitude__sam','longitude_dd_sam','longitude_deg_sam','biological_material_longitude_sam']\n",
    "        \n",
    "filtered_tag = np.concatenate((lat_lon_tag,lat_tag,lon_tag))\n",
    "\n",
    "for samples in dask.itertuples(index=False):\n",
    "     \n",
    "    # Problem of serialization between dask partition / pandas\n",
    "    # attributes = samples.attributes\n",
    "    # attributes_df = pd.DataFrame.from_records(attributes)\n",
    "\n",
    "    attributes = samples.jattr\n",
    "\n",
    "    if(not attributes==\"\"):\n",
    "\n",
    "        attributes_df = pd.DataFrame(json.loads(attributes).items(),columns=['k', 'v'])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        attributes_df=pd.DataFrame()\n",
    "        print(\"ERROR EMPTY\")\n",
    "\n",
    "    if('k' in attributes_df):\n",
    "            \n",
    "            attributes_df = attributes_df.loc[attributes_df['k'].isin(filtered_tag)]\n",
    "\n",
    "            for index, attribute in attributes_df.iterrows():\n",
    "                \n",
    "                #store attributes keyword\n",
    "\n",
    "                #if attribute['k'] in dict_attributes_k :\n",
    "                #    dict_attributes_k[attribute['k']] = dict_attributes_k[attribute['k']] + 1\n",
    "                #else :\n",
    "                #    dict_attributes_k[attribute['k']] = 0\n",
    "                #    print(\"keyword\", attribute['k'])\n",
    "\n",
    "                ###########################################################################\n",
    "                # latitude - longitude in the same field\n",
    "\n",
    "                lat_lon_list = None\n",
    "\n",
    "                if attribute['k'] in lat_lon_tag : \n",
    "                    dic[\"Lat Lon Tag\"] += 1\n",
    "                if attribute['k'] in lat_tag:\n",
    "                    dic['Lat Tag'] += 1\n",
    "                if attribute['k'] in lon_tag:\n",
    "                    dic['Lon Tag'] += 1\n",
    "                if not ((attribute['k'] in lat_lon_tag) or (attribute['k'] in lat_tag) or (attribute['k'] in lon_tag)):\n",
    "                    dic['No Tag'] += 1\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAT LON PATTERNS TEST\n",
    "\n",
    "lat_lon_patterns = [\n",
    "    r\"(-?\\d+\\.\\d+ [NS]) (-?\\d+\\.\\d+ [WE])\",   # xx.xxx N xx.xxx W\n",
    "    r\"(-?\\d+\\.\\d+' [NS]) (-?\\d+\\.\\d+' [WE])\", # xx.xxx' N xx.xxx' W\n",
    "    r\"(-?\\d+\\.\\d+° [NS]) (-?\\d+\\.\\d+° [WE])\", # xx.xxx° N xx.xxx° W\n",
    "    r'(-?\\d+ [NS]) (-?\\d+ [WE])',             # xx N xx W\n",
    "    r'(-?\\d+\\.\\d+ [NS]) (-?\\d+ [WE])',        # xx.xxx N xx W\n",
    "    r'(-?\\d+ [NS]) (-?\\d+\\.\\d+ [WE])',        # xx N xx.xxx W \n",
    "    r'(-?\\d+\\.\\d+) (-?\\d+\\.\\d+)',             # xx.xxx xx.xxx\n",
    "    r'(-?\\d+\\.\\d+°) (-?\\d+\\.\\d+°)',           # xx.xxx° xx.xxx°\n",
    "    r\"(-?\\d+\\.\\d+') (-?\\d+\\.\\d+')\",           # xx.xxx' xx.xxx'\n",
    "]\n",
    "\n",
    "dic = {\n",
    "    \"Match\"       :0,\n",
    "    \"Alpha String\":0,\n",
    "    \"No Match\"   :0,\n",
    "    \"Tot\"         :0\n",
    "}\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# \n",
    "# selected tag\n",
    "#\n",
    "\n",
    "lat_lon_tag = ['lat_lon_sam_s_dpl34','lat_lon_sam_s_dpl1','geographic_location__latitude_and_longitude__sam','geographic_location__latitudeandlongitude__sam','latitude__and_longitude_sam','latitude_and_longitude_sam','lattitude_and_logitude_sam','lat_lon_sam','latlon_sam','location_coordinates_sam','other_gps_coordinates_sam','lat_lon_dms_sam','lat_long_correct_sam','lat_lon_run']\n",
    "\n",
    "lat_tag = ['geographic_location__latitude__sam_s_dpl4','lat_lon_sam_s_dpl1','latitude_sam','lat_sam','geographic_location__latitude__sam','geographiclocation_latitude__sam','latitude_dd_sam','latitude_deg_sam','biological_material_latitude_sam']\n",
    "\n",
    "lon_tag = ['geographic_location__longitude__sam_s_dpl5','longitude_sam','lon_sam','geographic_location__longitude__sam','longitude_dd_sam','longitude_deg_sam','biological_material_longitude_sam']\n",
    "        \n",
    "filtered_tag = np.concatenate((lat_lon_tag,lat_tag,lon_tag))\n",
    "\n",
    "for samples in dask.itertuples(index=False):\n",
    "     \n",
    "    # Problem of serialization between dask partition / pandas\n",
    "    # attributes = samples.attributes\n",
    "    # attributes_df = pd.DataFrame.from_records(attributes)\n",
    "\n",
    "    attributes = samples.jattr\n",
    "\n",
    "    if(not attributes==\"\"):\n",
    "\n",
    "        attributes_df = pd.DataFrame(json.loads(attributes).items(),columns=['k', 'v'])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        attributes_df=pd.DataFrame()\n",
    "        print(\"ERROR EMPTY\")\n",
    "\n",
    "    if('k' in attributes_df):\n",
    "            \n",
    "            attributes_df = attributes_df.loc[attributes_df['k'].isin(filtered_tag)]\n",
    "\n",
    "            for index, attribute in attributes_df.iterrows():\n",
    "                \n",
    "                #store attributes keyword\n",
    "\n",
    "                #if attribute['k'] in dict_attributes_k :\n",
    "                #    dict_attributes_k[attribute['k']] = dict_attributes_k[attribute['k']] + 1\n",
    "                #else :\n",
    "                #    dict_attributes_k[attribute['k']] = 0\n",
    "                #    print(\"keyword\", attribute['k'])\n",
    "\n",
    "                ###########################################################################\n",
    "                # latitude - longitude in the same field\n",
    "\n",
    "                lat_lon_list = None\n",
    "\n",
    "                if attribute['k'] in lat_lon_tag :\n",
    "\n",
    "                    dic['Tot'] += 1\n",
    "                    \n",
    "                    lat_lon_src = attribute['k']\n",
    "\n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lat_lon = attribute['v'][0]\n",
    "                    else:\n",
    "                        lat_lon = attribute['v']\n",
    "\n",
    "                    if not is_alpha_string(str(lat_lon)):\n",
    "                        str_lat_lon = str(lat_lon)\n",
    "                        for pattern in lat_lon_patterns:\n",
    "                            if re.match(pattern,str_lat_lon):\n",
    "                                dic['Match'] += 1\n",
    "                                lat_lon_list = re.match(pattern,str_lat_lon)\n",
    "                        if lat_lon_list is None:\n",
    "                            dic['No Match'] += 1\n",
    "                    else:\n",
    "                        dic['Alpha String'] += 1\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAT & LON PATTERNS TEST\n",
    "\n",
    "lat_patterns = [\n",
    "    r\"(-?\\d+\\.\\d+ [NS])\",   # xx.xxx N xx.xxx W\n",
    "    r\"(-?\\d+\\.\\d+' [NS])\",  # xx.xxx' N xx.xxx' W\n",
    "    r\"(-?\\d+\\.\\d+° [NS])\",  # xx.xxx° N xx.xxx° W\n",
    "    r'(-?\\d+ [NS])',             # xx N xx W\n",
    "    r'(-?\\d+\\.\\d+ [NS])',        # xx.xxx N xx W\n",
    "    r'(-?\\d+\\.\\d+)',             # xx.xxx xx.xxx\n",
    "    r'(-?\\d+\\.\\d+°)',           # xx.xxx° xx.xxx°\n",
    "    r\"(-?\\d+\\.\\d+')\",           # xx.xxx' xx.xxx'\n",
    "]\n",
    "\n",
    "lon_patterns = [\n",
    "    r\"(-?\\d+\\.\\d+ [WE])\",\n",
    "    r\"(-?\\d+\\.\\d+' [WE])\",\n",
    "    r\"(-?\\d+\\.\\d+° [WE])\",\n",
    "    r\"(-?\\d+ [WE])\",\n",
    "    r\"(-?\\d+\\.\\d+ [WE])\",\n",
    "    r\"(-?\\d+\\.\\d+)\",\n",
    "    r\"(-?\\d+\\.\\d+°)\",\n",
    "    r\"(-?\\d+\\.\\d+')\"\n",
    "]\n",
    "\n",
    "dic = {\n",
    "    \"Lon & Lat Match\":0,\n",
    "    \"Lon Match\"      :0,\n",
    "    \"Lat Match\"      :0,\n",
    "    \"No Match\"       :0,\n",
    "    \"Incomplete\"     :0\n",
    "}\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# \n",
    "# selected tag\n",
    "#\n",
    "\n",
    "lat_lon_tag = ['lat_lon_sam_s_dpl34','lat_lon_sam_s_dpl1','geographic_location__latitude_and_longitude__sam','geographic_location__latitudeandlongitude__sam','latitude__and_longitude_sam','latitude_and_longitude_sam','lattitude_and_logitude_sam','lat_lon_sam','latlon_sam','location_coordinates_sam','other_gps_coordinates_sam','lat_lon_dms_sam','lat_long_correct_sam','lat_lon_run']\n",
    "\n",
    "lat_tag = ['geographic_location__latitude__sam_s_dpl4','lat_lon_sam_s_dpl1','latitude_sam','lat_sam','geographic_location__latitude__sam','geographiclocation_latitude__sam','latitude_dd_sam','latitude_deg_sam','biological_material_latitude_sam']\n",
    "\n",
    "lon_tag = ['geographic_location__longitude__sam_s_dpl5','longitude_sam','lon_sam','geographic_location__longitude__sam','longitude_dd_sam','longitude_deg_sam','biological_material_longitude_sam']\n",
    "        \n",
    "filtered_tag = np.concatenate((lat_lon_tag,lat_tag,lon_tag))\n",
    "\n",
    "for samples in dask.itertuples(index=False):\n",
    "     \n",
    "    # Problem of serialization between dask partition / pandas\n",
    "    # attributes = samples.attributes\n",
    "    # attributes_df = pd.DataFrame.from_records(attributes)\n",
    "\n",
    "    attributes = samples.jattr\n",
    "    match_score = 0\n",
    "\n",
    "    if(not attributes==\"\"):\n",
    "\n",
    "        attributes_df = pd.DataFrame(json.loads(attributes).items(),columns=['k', 'v'])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        attributes_df=pd.DataFrame()\n",
    "        print(\"ERROR EMPTY\")\n",
    "\n",
    "    if('k' in attributes_df):\n",
    "            \n",
    "            attributes_df = attributes_df.loc[attributes_df['k'].isin(filtered_tag)]\n",
    "\n",
    "            for index, attribute in attributes_df.iterrows():\n",
    "\n",
    "                if attribute['k'] in lat_tag :\n",
    "                    match_score += 0.5\n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lat = attribute['v'][0]\n",
    "                    else:\n",
    "                        lat = attribute['v']\n",
    "\n",
    "                    if not is_alpha_string(str(lat)):\n",
    "                        str_lat = str(lat)\n",
    "                        for pattern in lat_patterns:\n",
    "                            if re.match(pattern,str_lat):\n",
    "                                match_score += 1\n",
    "                            \n",
    "                if attribute['k'] in lon_tag :\n",
    "                    match_score += 0.5\n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lon = attribute['v'][0]\n",
    "                    else:\n",
    "                        lon = attribute['v']\n",
    "\n",
    "                    if not is_alpha_string(str(lon)):\n",
    "                        str_lon = str(lon)\n",
    "                        for pattern in lon_patterns:\n",
    "                            if re.match(pattern,str_lon):\n",
    "                                match_score += 2\n",
    "    \n",
    "    if np.floor(match_score) != match_score :\n",
    "        dic['Incomplete'] += 1\n",
    "    elif match_score == 1:\n",
    "        dic['No Match'] += 1\n",
    "    elif match_score == 2:\n",
    "        dic['Lat Match'] += 1\n",
    "    elif match_score == 3:\n",
    "        dic['Lon Match'] += 1\n",
    "    elif match_score == 4:\n",
    "        dic['Lon & Lat Match'] += 1\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GeoSRA function\n",
    "\n",
    "def parse_geo_sra(pd_array_partition): \n",
    "\n",
    "    array_result = []\n",
    "    #i,n = 0,len(pd_array_partition)\n",
    "\n",
    "    for samples in pd_array_partition.itertuples(index=False):\n",
    "\n",
    "        # initialise variable\n",
    "\n",
    "        lat_lon_src = ''\n",
    "        lat_lon_raw = ''\n",
    "        lat_src = ''\n",
    "        lat_raw = ''\n",
    "\n",
    "        lon_src = ''\n",
    "        lon_raw = ''\n",
    "        \n",
    "        lat_lon = ''\n",
    "        lat = ''\n",
    "        lon = ''\n",
    "        lat_list = None\n",
    "        lon_list = None\n",
    "        lat_lon_list = None\n",
    "\n",
    "        latitude = None\n",
    "        longitude = None\n",
    "\n",
    "        latitude_precision = None\n",
    "        longitude_precision = None\n",
    "\n",
    "        has_latlon = False\n",
    "\n",
    "        geo_qual = 4.0\n",
    "\n",
    "        biome_num = None\n",
    "        ecoregion_id = None\n",
    "        ecoregion_name = None\n",
    "\n",
    "        #\n",
    "        # parse from metadata available\n",
    "        #\n",
    "\n",
    "        bioproject = samples.bioproject\n",
    "        acc = samples.acc\n",
    "        organism = samples.organism\n",
    "        assay_type = samples.assay_type\n",
    "        instrument = samples.instrument\n",
    "        librarylayout = samples.librarylayout\n",
    "        libraryselection = samples.libraryselection\n",
    "        librarysource = samples.librarysource\n",
    "        geo_loc_name_country_calc = samples.geo_loc_name_country_calc\n",
    "        geo_loc_name_country_continent_calc = samples.geo_loc_name_country_continent_calc\n",
    "        mbytes = samples.mbytes\n",
    "        mbases = samples.mbases\n",
    "\n",
    "        releasedate = None\n",
    "        if( not pd.isnull(samples.releasedate)) :\n",
    "            if(is_date(samples.releasedate)):\n",
    "                 releasedate = pd.to_datetime(samples.releasedate)\n",
    "\n",
    "        collection_date = None\n",
    "        if( not pd.isnull(samples.collection_date)) :\n",
    "            # multiple collection date are sometimes available for the same sample\n",
    "            # take the first one\n",
    "            if(is_date(samples.collection_date)):\n",
    "                collection_date = pd.to_datetime(samples.collection_date)\n",
    "\n",
    "        # \n",
    "        # reverse geocoding\n",
    "        #\n",
    "\n",
    "        rg_country_code = ''\n",
    "        rg_city = ''\n",
    "        rg_country = ''\n",
    "\n",
    "        #\n",
    "        # parse from attributes metadata available\n",
    "        #\n",
    "\n",
    "        lat_lon_patterns = [\n",
    "            r\"(-?\\d+\\.\\d+ [NS]) (-?\\d+\\.\\d+ [WE])\",   # xx.xxx N xx.xxx W\n",
    "            r\"(-?\\d+\\.\\d+' [NS]) (-?\\d+\\.\\d+' [WE])\", # xx.xxx' N xx.xxx' W\n",
    "            r\"(-?\\d+\\.\\d+° [NS]) (-?\\d+\\.\\d+° [WE])\", # xx.xxx° N xx.xxx° W\n",
    "            r'(-?\\d+ [NS]) (-?\\d+ [WE])',             # xx N xx W\n",
    "            r'(-?\\d+\\.\\d+ [NS]) (-?\\d+ [WE])',        # xx.xxx N xx W\n",
    "            r'(-?\\d+ [NS]) (-?\\d+\\.\\d+ [WE])',        # xx N xx.xxx W \n",
    "            r'(-?\\d+\\.\\d+) (-?\\d+\\.\\d+)',             # xx.xxx xx.xxx\n",
    "            r'(-?\\d+\\.\\d+°) (-?\\d+\\.\\d+°)',           # xx.xxx° xx.xxx°\n",
    "            r\"(-?\\d+\\.\\d+') (-?\\d+\\.\\d+')\",           # xx.xxx' xx.xxx'\n",
    "        ]\n",
    "\n",
    "        lat_patterns = [\n",
    "            r\"(-?\\d+\\.\\d+ [NS])\",   # xx.xxx N xx.xxx W\n",
    "            r\"(-?\\d+\\.\\d+' [NS])\",  # xx.xxx' N xx.xxx' W\n",
    "            r\"(-?\\d+\\.\\d+° [NS])\",  # xx.xxx° N xx.xxx° W\n",
    "            r'(-?\\d+ [NS])',             # xx N xx W\n",
    "            r'(-?\\d+\\.\\d+ [NS])',        # xx.xxx N xx W\n",
    "            r'(-?\\d+\\.\\d+)',             # xx.xxx xx.xxx\n",
    "            r'(-?\\d+\\.\\d+°)',           # xx.xxx° xx.xxx°\n",
    "            r\"(-?\\d+\\.\\d+')\",           # xx.xxx' xx.xxx'\n",
    "        ]\n",
    "\n",
    "        lon_patterns = [\n",
    "            r\"(-?\\d+\\.\\d+ [WE])\",\n",
    "            r\"(-?\\d+\\.\\d+' [WE])\",\n",
    "            r\"(-?\\d+\\.\\d+° [WE])\",\n",
    "            r\"(-?\\d+ [WE])\",\n",
    "            r\"(-?\\d+\\.\\d+ [WE])\",\n",
    "            r\"(-?\\d+\\.\\d+)\",\n",
    "            r\"(-?\\d+\\.\\d+°)\",\n",
    "            r\"(-?\\d+\\.\\d+')\"\n",
    "        ]\n",
    "\n",
    "        ###########################################################################\n",
    "        # \n",
    "        # selected tag\n",
    "        #\n",
    "\n",
    "        lat_lon_tag = ['lat_lon_sam_s_dpl34','lat_lon_sam_s_dpl1','geographic_location__latitude_and_longitude__sam','geographic_location__latitudeandlongitude__sam','latitude__and_longitude_sam','latitude_and_longitude_sam','lattitude_and_logitude_sam','lat_lon_sam','latlon_sam','location_coordinates_sam','other_gps_coordinates_sam','lat_lon_dms_sam','lat_long_correct_sam','lat_lon_run']\n",
    "\n",
    "        lat_tag = ['geographic_location__latitude__sam_s_dpl4','lat_lon_sam_s_dpl1','latitude_sam','lat_sam','geographic_location__latitude__sam','geographiclocation_latitude__sam','latitude_dd_sam','latitude_deg_sam','biological_material_latitude_sam']\n",
    "\n",
    "        lon_tag = ['geographic_location__longitude__sam_s_dpl5','longitude_sam','lon_sam','geographic_location__longitude__sam','longitude_dd_sam','longitude_deg_sam','biological_material_longitude_sam']\n",
    "        \n",
    "        filtered_tag = np.concatenate((lat_lon_tag,lat_tag,lon_tag))\n",
    "\n",
    "        # Problem of serialization between dask partition / pandas\n",
    "        # attributes = samples.attributes\n",
    "        # attributes_df = pd.DataFrame.from_records(attributes)\n",
    "\n",
    "\n",
    "        attributes = samples.jattr\n",
    "\n",
    "        if(not attributes==\"\"):\n",
    "\n",
    "            attributes_df = pd.DataFrame(json.loads(attributes).items(),columns=['k', 'v'])\n",
    "\n",
    "        else:\n",
    "            attributes_df=pd.DataFrame()\n",
    "            print(\"ERROR EMPTY\")\n",
    "\n",
    "        ###########################################################################\n",
    "        \n",
    "        if('k' in attributes_df):\n",
    "            \n",
    "            attributes_df = attributes_df.loc[attributes_df['k'].isin(filtered_tag)]\n",
    "\n",
    "            for index, attribute in attributes_df.iterrows():\n",
    "                \n",
    "                #store attributes keyword\n",
    "\n",
    "                #if attribute['k'] in dict_attributes_k :\n",
    "                #    dict_attributes_k[attribute['k']] = dict_attributes_k[attribute['k']] + 1\n",
    "                #else :\n",
    "                #    dict_attributes_k[attribute['k']] = 0\n",
    "                #    print(\"keyword\", attribute['k'])\n",
    "\n",
    "                ###########################################################################\n",
    "                # latitude - longitude in the same field\n",
    "\n",
    "\n",
    "                if attribute['k'] in lat_lon_tag :\n",
    "                    \n",
    "                    lat_lon_src = attribute['k']\n",
    "\n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lat_lon = attribute['v'][0]\n",
    "                        lat_lon_raw = attribute['v'][0]\n",
    "                    else:\n",
    "                        lat_lon = attribute['v']\n",
    "                        lat_lon_raw = attribute['v']\n",
    "\n",
    "                    if not is_alpha_string(str(lat_lon)):\n",
    "                        for pattern in lat_lon_patterns:\n",
    "                            if re.match(pattern,str(lat_lon)):\n",
    "                                lat_lon_list = re.match(pattern,str(lat_lon))\n",
    "\n",
    "                ###########################################################################\n",
    "\n",
    "                ###########################################################################\n",
    "                # latitude - longitude in separated fields\n",
    "\n",
    "                # latitude\n",
    "\n",
    "                if attribute['k'] in lat_tag :\n",
    "                    \n",
    "                    lat_src = attribute['k']\n",
    "\n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lat = attribute['v'][0]\n",
    "                        lat_raw = attribute['v'][0]\n",
    "                    else:\n",
    "                        lat = attribute['v']\n",
    "                        lat_raw = attribute['v']\n",
    "\n",
    "                    if lat_lon_src == '':\n",
    "                        lat_lon_src = lat_src\n",
    "                        lat_lon_raw = lat_raw\n",
    "                    else:\n",
    "                        lat_lon_src = lat_src + \" \" + lat_lon_src\n",
    "                        lat_lon_raw = str(lat_raw) + \" \" + lat_lon_raw\n",
    "\n",
    "                    if not is_alpha_string(str(lat)) :\n",
    "                        for pattern in lat_patterns:\n",
    "                            if re.match(pattern,str(lat)):\n",
    "                                lat_list = re.match(pattern,str(lat))\n",
    "\n",
    "                # longitude\n",
    "                \n",
    "                if attribute['k'] in lon_tag :\n",
    "                    \n",
    "                    lon_src = attribute['k']\n",
    "                    \n",
    "                    if(isinstance(attribute['v'], list)):\n",
    "                        lon = attribute['v'][0]\n",
    "                        lon_raw = attribute['v'][0]\n",
    "                    else:\n",
    "                        lon = attribute['v']\n",
    "                        lon_raw = attribute['v']\n",
    "                    \n",
    "                    if lat_lon_src == '':\n",
    "                        lat_lon_src = lon_src\n",
    "                        lat_lon_raw = lon_raw\n",
    "                    else:\n",
    "                        lat_lon_src = lat_lon_src + \" \" + lon_src\n",
    "                        lat_lon_raw = lat_lon_raw + \" \" + str(lon_raw)\n",
    "\n",
    "                    if not is_alpha_string(str(lon)) :\n",
    "                        for pattern in lon_patterns:\n",
    "                            if re.match(pattern,str(lon)):\n",
    "                                lon_list = re.match(pattern,str(lon))\n",
    "                \n",
    "                ###########################################################################\n",
    "\n",
    "            ###########################################################################\n",
    "                \n",
    "            #\n",
    "            # lat_lon_parser\n",
    "            #\n",
    "\n",
    "            if lat_lon_list is not None :\n",
    "\n",
    "                try :\n",
    "                    latitude = parse(lat_lon_list.group(1))\n",
    "                    longitude = parse(lat_lon_list.group(2))\n",
    "                    latitude_precision = decimal_precision(latitude)\n",
    "                    longitude_precision = decimal_precision(longitude)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if (lon_list is not None) & (lat_list is not None) & (str(lon).count(\".\") <=1) & (str(lat).count(\".\") <=1) :\n",
    "\n",
    "                if (lon_list.group(1) is not None) & (lat_list.group(1) is not None) :\n",
    "\n",
    "                    try :\n",
    "                        latitude = parse(lat_list.group(1))\n",
    "                        longitude = parse(lon_list.group(1))\n",
    "                        latitude_precision = decimal_precision(latitude)\n",
    "                        longitude_precision = decimal_precision(longitude)\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "            #\n",
    "            # reverse geocoding\n",
    "            #\n",
    "            \n",
    "            if((latitude is not None) & (longitude is not None)):\n",
    "                coordinates = [(latitude,longitude)]\n",
    "                has_latlon=True\n",
    "                rgeocode = rg.search(coordinates)\n",
    "                \n",
    "                if(len(rgeocode)>0):\n",
    "                    rg_country_code = rgeocode[0]['country_code']\n",
    "                    rg_city = rgeocode[0]['city']\n",
    "                    rg_country = rgeocode[0]['country']\n",
    "            \n",
    "            #\n",
    "            # GEO_QUAL calc\n",
    "            #\n",
    "            \n",
    "            thr_prec = 2\n",
    "            if ((latitude is not None) & (longitude is not None)):\n",
    "                if ((latitude_precision > thr_prec) | (longitude_precision > thr_prec)):\n",
    "                    geo_qual = 1.3\n",
    "                    if not country_is_na(geo_loc_name_country_calc):\n",
    "                        geo_qual = 1.2\n",
    "                        if match_countries(geo_loc_name_country_calc,rg_country):\n",
    "                            geo_qual = 1.1\n",
    "                        else:\n",
    "                            if reverse_lat_lon_test(latitude,longitude,geo_loc_name_country_calc):\n",
    "                                longitude,latitude = latitude,longitude\n",
    "                                geo_qual = 1.15\n",
    "                else:\n",
    "                    geo_qual = 2.3\n",
    "                    if not country_is_na(geo_loc_name_country_calc):\n",
    "                        geo_qual = 2.2\n",
    "                        if match_countries(geo_loc_name_country_calc,rg_country):\n",
    "                            geo_qual = 2.1\n",
    "                        else:\n",
    "                            if reverse_lat_lon_test(latitude,longitude,geo_loc_name_country_calc):\n",
    "                                longitude,latitude = latitude,longitude\n",
    "                                geo_qual = 2.15\n",
    "\n",
    "            elif (((latitude is None) | (longitude is None)) & (not country_is_na(geo_loc_name_country_calc))):\n",
    "                geo_qual = 3.0\n",
    "            \n",
    "        # append result\n",
    "        array_result.append([bioproject,acc,organism,assay_type,instrument,librarylayout,libraryselection,librarysource,geo_loc_name_country_calc,geo_loc_name_country_continent_calc,mbytes,mbases,releasedate,collection_date,lat_lon_src,lat_lon_raw,latitude,longitude,latitude_precision,has_latlon,longitude_precision,rg_country_code,rg_city,rg_country,geo_qual,biome_num,ecoregion_id,ecoregion_name])\n",
    "\n",
    "        #i+=1\n",
    "        #print(f'{i*100/n} %')\n",
    "\n",
    "    columns = ['bioproject', 'acc','organism','assay_type','instrument','librarylayout','libraryselection','librarysource','geo_loc_name_country_calc','geo_loc_name_country_continent_calc','mbytes','mbases','releasedate', 'collection_date','lat_lon_src','lat_lon_raw','latitude','longitude','latitude_precision','has_latlon','longitude_precision','rg_country_code','rg_city','rg_country','GEO_QUAL']\n",
    "    dtype = [str,str,str,str,str,str,str,str,str,str,int,int,datetime.date,datetime.date,str,str,float,float,float,float,bool,str,str,str,float,int,int,str]\n",
    "    cdt={i[0]: i[1] for i in zip(columns, dtype)}        \n",
    "    pd_result = pd.DataFrame(data=array_result,columns=list(cdt))\n",
    "\n",
    "    return pd_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSING DATAS\n",
    "t1 = time.time()\n",
    "p.set_done(False)\n",
    "\n",
    "thr = threading.Thread(target=graph.animate_string_execution(\"Exécution de parse_geo_sra\"))\n",
    "thr.start()\n",
    "\n",
    "try:\n",
    "\n",
    "    pd_output = dask.map_partitions(parse_geo_sra, meta=meta_df).compute(scheduler='multiprocessing')\n",
    "\n",
    "    p.set_done(True)\n",
    "\n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Exécution de parse_geo_sra réussie\",time=t,error=False)\n",
    "    \n",
    "except Exception as e:\n",
    "\n",
    "    p.set_done(True)\n",
    "    \n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur d'exécution de parse_geo_sra\",time=t,error_type=e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m# Enregistrement des métadonnées       \n",
      "\u001b[91m#[0.0s]---Erreur d'enregistrement\n",
      "name 'type_open' is not defined\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save parsed datas\n",
    "t1 = time.time()\n",
    "p.set_done(False)\n",
    "\n",
    "thr = threading.Thread(target=graph.animate_string_execution(\"Enregistrement des métadonnées\"))\n",
    "thr.start()\n",
    "\n",
    "try:\n",
    "\n",
    "    if type_open == \"dist\":\n",
    "        print('to do')\n",
    "    \n",
    "    elif type_open == \"loc\":\n",
    "        print('to do')\n",
    "    \n",
    "    p.set_done(True)\n",
    "    \n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Enregistrement réussi\",time=t,error=False)\n",
    "\n",
    "except Exception as e:\n",
    "    \n",
    "    p.set_done(True)\n",
    "\n",
    "    print('')\n",
    "    t2 = time.time()\n",
    "    t  = np.round(t2 - t1,decimals=1)\n",
    "    graph.print_cell_msg(\"Erreur d'enregistrement\",time=t,error_type=e)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
